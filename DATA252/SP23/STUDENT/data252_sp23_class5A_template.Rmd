---
title: 'DATA252: Variable Selection and Regularization'
output:
  html_document:
    collapsed: no
    toc: yes
    toc_depth: 5
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '5'
---

## Learning Objectives

In this lesson students will learn how to implement...

* Best Subset Selection
* Stepwise Selection (Backward and Forward)
* Ridge Regression
* LASSO Regression

## Ex: Health and Biostatistics

Use the following data for these examples: 

```{r warning=FALSE, message=FALSE}
library(tidyverse)

#install.packages("faraway")
library(faraway)
set.seed(1212)

data("fat")
head(fat)
```

Read about the `fat` dataset

```{r}
help(fat)
```

Our goal is to predict percent of body fat from Brozek's equation. 

## Best Subset Selection

There are $2^n$ possible models to explore.  Best Subset Selection finds the best set of variables for a model of a given size. 

```{r}
#install.packages("leaps")
library(leaps) 

### BEST SUBSET

bestsub.model <- regsubsets(brozek ~ age + 
                              weight + height + 
                              adipos + 
                              neck + chest + 
                              abdom + hip + 
                              thigh + knee + 
                              ankle +biceps + 
                              forearm + wrist, 
                            data = fat, nvmax = 14)

# nvmax: max size of subsets

summary(bestsub.model)

```
### Model Metrics

```{r}
#Performance measures
best14<-data.frame(nvars=1:14,
  Cp     = ## YOUR CODE HERE ##,
  r2     = ## YOUR CODE HERE ##,
  Adj_r2 = ## YOUR CODE HERE ##,
  BIC    =## YOUR CODE HERE ##)%>%
  gather(metric, value, -c(nvars))

ggplot(best14, aes(x=nvars, y=value, color=metric))+
  geom_line()+
  facet_grid(metric~., scales = "free")

which.min(summary(bestsub.model)$cp)
which.max(summary(bestsub.model)$rsq)
which.max(summary(bestsub.model)$adjr2)
which.min(summary(bestsub.model)$bic)
```

### Cross-Validation

```{r}
##jack-knife validation (leave-one-out)

##Function to get predictions from the regsubset function 
predict.regsubsets <- function(object, newdata, id,...){
  form  <- as.formula(object$call[[2]])
  mat   <- model.matrix(form, newdata)
  coefi <- coef(object, id=id)
  mat[, names(coefi)]%*%coefi
}

#store the prediction error n=252
jk.errors <- matrix(NA, 252, 14) 

for (k in 1:252){
  #uses regsubsets in the data with 1 observation removed 
  best.model.cv <- regsubsets(brozek ~ age + 
                                weight + height + 
                                adipos + neck + chest + 
                                abdom + hip + thigh +
                                knee + ankle + biceps + 
                                forearm + wrist, 
                              data = fat[-k,],          #removes the kth obsv
                              nvmax = 14) 
  
  #Models with 1, 2 ,...14 predictors
  for (i in 1:14){
    #that was left out
    pred <- predict.regsubsets(best.model.cv,                 #prediction in the obsv 
                    fat[k,], 
                    id=i)
    jk.errors[k,i] <- (fat$brozek[k]-pred)^2       #error in the obsv 
  }
}

mse.models <- apply(jk.errors, 2, mean)            #MSE estimation 
plot(mse.models ,                              #Plot with MSEs
     pch=19, type="b",
     xlab="nr predictors",
     ylab="MSE")
```

### Final Model

The best final model has 7 variables.

```{r}
coef(bestsub.model, id=7)
```
## Backward Selection

```{r}
model.bwrd <- ## YOUR CODE HERE ##
summary(model.bwrd)
```
### Final Model

```{r}
which.min(summary(model.bwrd)$cp)
## YOUR CODE HERE ##
```

## Forward Selection

```{r}
### forward
model.fwrd <- ## YOUR CODE HERE ##
summary(model.fwrd)
```

### Final Model

```{r}
which.min(summary(model.fwrd)$cp)
## YOUR CODE HERE ##
```

#### Questions

* How would this final model change if you used a different model metric?

## Ridge Regression

Ridge regression minimizes the residual sum of squares with an $L2$ penalty.

$$\sum_{i=1}^n(y_i-\beta_0-\sum_{j=1}^p\beta_jx_{ij})^2+\lambda\sum_{j=1}^p\beta_j^2$$

Big Ideas: 

* This shrinks $\beta_j$ coefficients toward zero
* Stills works with multicollinarity

```{r}
### ridge
#install.packages(c("glmnet", "faraway"))
library(glmnet)   #function for ridge regression
library(faraway) #has the dataset fat
set.seed(1233)

#RIDGE REGRESSION

#we need to define the model equation
X <- model.matrix(brozek ~  age + weight + 
                    height + adipos +
                    neck + chest + 
                    abdom + hip + thigh +
                    knee + ankle + 
                    biceps + forearm + 
                    wrist, data=fat)[,-1]
#and the outcome
Y <- fat[,"brozek"] 

#Penalty type (alpha=0 is ridge)
cv.lambda <- ## YOUR CODE HERE ##  

plot(cv.lambda)                                 #MSE for several lambdas
cv.lambda$lambda.min 

#ridge path
plot(cv.lambda$glmnet.fit, 
     "lambda", label=FALSE)

```

When lambda is high, the coefficients shrink toward zero.

### Final Model

```{r}
lmin        <- cv.lambda$lambda.min

ridge.model <- ## YOUR CODE HERE ##
ridge.model$beta
```

### Compare to OLS

```{r}
ols.regression <- lm(brozek ~  age + weight +                 #OLS 
                       height + adipos +
                       neck + chest + 
                       abdom + hip + thigh +
                       knee + ankle + 
                       biceps + forearm + 
                       wrist, data=fat)
#OLS vs RIDGE
round(cbind(OLS = coef(ols.regression), 
            ridge = c(ridge.model$a0,                              #intercept for
                      as.vector(ridge.model$beta))),4)
```

## LASSO Regression

Ridge regression minimizes the residual sum of squares with an $L1$ penalty.

$$\sum_{i=1}^n(y_i-\beta_0-\sum_{j=1}^p\beta_jx_{ij})^2+\lambda\sum_{j=1}^p|\beta_j|$$

Big Ideas: 

* $\beta_j$ become zero
* Works like variable selection

```{r}
#Penalty type (alpha=1 is lasso 
#and alpha=0 is the ridge)
cv.lambda.lasso <- ## YOUR CODE HERE ##
  
plot(cv.lambda.lasso)                        #MSE for several lambdas
cv.lambda.lasso    
#Lasso path
plot(cv.lambda.lasso$glmnet.fit, 
     "lambda", label=FALSE)
```

### Final Model

```{r}
l.lasso.min <- cv.lambda.lasso$lambda.min
l.lasso.min

lasso.model <- ## YOUR CODE HERE ##
lasso.model$beta 
```

Source Examples from: 

<https://bookdown.org/tpinto_home/Regularisation/lasso-regression.html>
