---
title: 'DATA252: Review of Simple Linear Regression'
author: "Kitada Smalley"
output:
  pdf_document:
    toc: yes
    toc_depth: '5'
  html_document:
    collapsed: no
    toc: yes
    toc_depth: 5
    toc_float: yes
---

## Learning Objectives

In this lesson students will review the main concepts of Simple Linear Regression. 

* How to fit a model
* Graphics a model
* Checking model fit and diagnostics
* Model inference

## I. Motivating Example

### Import the Data

These laptop price data come from Kaggle. 

```{r warning=FALSE, message=FALSE}
### use raw file from github
laptop_price <- read.csv("https://raw.githubusercontent.com/kitadasmalley/DATA252/main/Data/laptop_price.csv")

```


### Step Zero: Wrangle Data

```{r warning=FALSE, message=FALSE}
## TIDYVERSE
#install.packages("tidyverse")
library(tidyverse)

### convert to dollars
laptop_price<-laptop_price%>%
  mutate(Price_dollar=Price_euros*1.09)

# TAKE OFF LABELS for GB
unique(laptop_price$Ram)

#str_remove()
laptop_price$Ram_GB<-str_remove(laptop_price$Ram, "GB")
laptop_price$Ram_GB<-as.numeric(laptop_price$Ram_GB)
unique(laptop_price$Ram_GB)
```

### Step One: Look at the Data

* Explanatory (X): RAM in GB
* Repsonse (Y): Price in Dollars

#### Make a Scatterplot

```{r}
# SCATTERPLOT
ggplot(data=laptop_price, 
       aes(y=Price_dollar, x=Ram_GB))+
  geom_point(alpha=.25)

# JITTER ADDS NOISE
ggplot(data=laptop_price, 
       aes(y=Price_dollar, x=Ram_GB))+
  geom_jitter(alpha=.25)
```

### Step Two: Assume a Model Form 

#### Non-Parametric 

```{r}
ggplot(data=laptop_price, 
       aes(y=Price_dollar, x=Ram_GB))+
  geom_jitter(alpha=.25)+
  geom_smooth(method="loess", se=FALSE)
```

#### Parametric 

```{r}
ggplot(data=laptop_price, 
       aes(y=Price_dollar, x=Ram_GB))+
  geom_jitter(alpha=.25)+
  geom_smooth(method="lm", se=FALSE)
```

We're going to go with a simple linear model

## II. Fitting A Simple Linear Regression (SLR) Model 

This model has the form: 
$$Y_i=\beta_0+\beta_1 x_i +\varepsilon_i$$

This might look familar to $Y=mx+b$, which was called slope-intercept form.

### Step Three: Estimate Model Coefficients 

```{r}
mod1<-lm(Price_dollar~Ram_GB, data=laptop_price)
summary(mod1)
```

We can estimate the parameters: 

* Y-intercept: $\hat{\beta}_0 = 291.57$
* Slope: $\hat{\beta}_1 = 111.34$

### Step Four: Error Estimation 

#### Standard Errors
We can estimate the error associated estimating these parameters: 
* Standard Error for Y-intercept: $SE(\hat{\beta}_0) = 27.26$
* Standard Error for Slope: $SE(\hat{\beta}_1) = 2.78$

#### Estimating the variance, $\sigma^2$

We can estimate the $\sigma^2$ with $s^2$: 
$$s^2=\frac{1}{n-2}\sum_{i=1}^n(Y_i-\hat{Y}_i)^2$$


The quantity $s^2$ is called the **mean squared error (MSE)**.  

The denominator $n-2$ is referred to as the residual regrees of freedom.

Rather than $s^2$ we often use $s$: 
$$s=\sqrt{\frac{1}{n-2}\sum_{i=1}^n(Y_i-\hat{Y}_i)^2}$$

We can find this value in the R output, but we can also check it: 

```{r}
sqrt(sum(mod1$residuals^2)/1301)
```

We can also use the `broom` package!  Using the `glance()` function in `broom` accesses the model metrics. 

```{r}
library(broom)

mod1%>%
  glance()%>%
  pull(sigma)
```


## III. Assessing Model Assumptions

We assume that the $x_i$'s are non-random (or observable with negligible error) and that the $\epsilon_i$'s are random such that 
* $E[\epsilon_i]=0$ (mean 0)
* $Var[\epsilon_i]=\sigma^2$ for all $i=1, ..., n$ (homogeneity)
* $Cor(x_i, x_j)= 0 $ for all $i \neq j$ (uncorrelated, but does not necessarily imply independence)

### Tools for assessment: 

* Residual Plot 
* QQ (Quantile-Quantile) Plot 
* Leverage vs Residual Plot 

#### Residual Plot from Scratch
```{r}
ggplot(data=laptop_price, aes(x=Ram_GB, y=mod1$residuals))+
  geom_jitter(alpha=.25)+
  geom_hline(yintercept = 0, lty=2, lwd=1, color="blue")
```

#### QQ Plot from Scratch

```{r}
qqnorm(mod1$residuals)
qqline(mod1$residuals)
```

#### Plot Function

```{r}
plot(mod1)
```


### Remove the Outlier

```{r}
laptop_price_wo<-laptop_price%>%
  filter(Ram_GB<40)

mod2<-lm(Price_dollar~Ram_GB, data=laptop_price_wo)
summary(mod2)

plot(mod2)
```


## IV. Inference


### Step Five: Hypothesis Testing for Parameters

#### Hypothesis Testing for Slope 

##### Hypotheses: 

* Null Hypothesis: $H_0: \beta_1 = 0$ 
* Alternative Hypothesis: 
+ $H_A: \beta_1 \neq 0$ (two-sided)
+ $H_A: \beta_1 > 0$ (one-sided upper)
+ $H_A: \beta_1 < 0$ (one-sided lower)

##### Test Statistic: 

$$t=\frac{\hat{\beta}_1}{SE({\hat{\beta}_1})}$$

where 
$$SE(\hat{\beta}_1)=\sqrt{\frac{s^2}{\sum_{i=1}^n (x_i-\bar{x})^2}}$$

##### Reference Distribution:

T-distribution with $n-2$ degrees of freedom. 

##### P-value: 
```{r}
beta1 <- mod2$coefficients[2]
beta1

beta1SE <- sqrt((sum(mod2$residuals^2))/1300/sum((laptop_price_wo$Ram_GB - mean(laptop_price_wo$Ram_GB))^2))
beta1SE

testStat<- beta1/beta1SE

pt(-abs(testStat), 1300, lower.tail=TRUE)*2
```

### Step Six: Confidence Interval for Parameters

Confidence intervals take the form 

$$\text{Point Estimate} \pm \text{Critical Value} \times \text{Standard Error}$$

#### Confidence Interval for Slope 

$$\hat{\beta}_1 \pm t_{df=n-2, \alpha/2} \times \sqrt{\frac{s^2}{\sum_{i=1}^n (x_i-\bar{x})^2}}$$

##### Point Estiamte: $\hat{\beta}_1$

```{r}
beta1 <- mod2$coefficients[2]
beta1
```


##### Critical Value: $t_{df=n-2, \alpha/2}$

We define the significance level $\alpha$, therefore we need to find the $1-\alpha/2$ quantile.  Typically, the default significance level used is $\alpha=0.05$, thus the percentile we look for is $97.5%$. 

Recall that the degrees of freedom are $n-2$.

```{r}
qt(0.975, df=1302-2)
```

##### Standard Error: $SE(\hat{\beta}_1)=\sqrt{\frac{s^2}{\sum_{i=1}^n (x_i-\bar{x})^2}}$

```{r}
beta1SE <- sqrt((sum(mod2$residuals^2))/1300/sum((laptop_price_wo$Ram_GB - mean(laptop_price_wo$Ram_GB))^2))
beta1SE
```


#### Pull this all together: 
```{r}
beta1 +c(-1,1)*qt(0.975, df=1302-2)*beta1SE
```

We can check this with the confint function in R: 

```{r}
confint(mod2)
```


## V. Prediction 

### Step Seven: Confidence and Prediction Intervals for an Observation 

#### Using the Model for Prediction 
```{r}
newdata=data.frame(Ram_GB=10)
predict(mod2, newdata)
```

#### Confidence Interval for a mean response
```{r}
predict(mod2, newdata, interval="confidence")
```

#### Prediction Interval for a new response 
```{r}
predict(mod2, newdata, interval="predict")
```

#### Adding Confidence and Prediction Bands
```{r}
confBand<-predict(mod2, interval="confidence")
predBand<-predict(mod2, interval="predict")

colnames(predBand)<-c("fit2", "lwr2", "upr2")


newDF<-cbind(laptop_price_wo, confBand, predBand)

ggplot(newDF, aes(x=Ram_GB, y=Price_dollar))+
  geom_point(alpha=.3)+
  geom_abline(slope=mod2$coefficients[2], intercept=mod2$coefficients[1],
              color="blue", lty=2, lwd=1)+
  geom_line(aes(y=lwr), color="green", lty=2, lwd=1)+
  geom_line(aes(y=upr), color="green", lty=2, lwd=1)+
  geom_line(aes(y=lwr2), color="red", lty=2, lwd=1)+
  geom_line(aes(y=upr2), color="red", lty=2, lwd=1)+
  theme_bw()
```

## VI. Variability

### Step Eight and Nine: Analysis of Variance

```{r}
anova(mod2)
```

Researchers are often interested in the $R^2$ value, which is known as the coefficient of determination.

$$R^2=\frac{SS(Reg)}{SS(Tot)}=1-\frac{SS(Res)}{SS(Tot)}$$

In the case of simple linear regression we can also square the correlation. 

```{r}
r_xy<-cor(laptop_price_wo$Ram_GB, laptop_price_wo$Price_dollar, )
r_xy^2
```

We can also use the `broom()` function:

```{r}
mod2%>%
  glance()%>%
  pull(r.squared)
```

## VII. Training and Testing

It is circular logic to assess model fit on data used to create the model.  Instead, we will split the data into a training set and a testing set.  We fit the model using the training set only.  Then we use the test set data as a proxy for new observations to understand how the model will perform. 

```{r}
### TEST TRAIN

dim(laptop_price_wo)
# 1302

## TEST with 1/3 
1302/3
# 434

set.seed(123)
testInd<-sample(1:1302, size=434, replace=FALSE)
train<-laptop_price_wo[-testInd, ]
test<-laptop_price_wo[testInd, ]

## TRAIN
trainMod<-lm(Price_dollar~Ram_GB, data=train)
summary(trainMod)

## TEST
testPred<-predict(trainMod, test)

sqrt(mean((testPred-test$Price_dollar)^2))

#install.packages("caret")
library(caret)
RMSE(testPred, test$Price_dollar)
```







